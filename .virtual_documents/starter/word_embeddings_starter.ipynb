











# Install required libraries (run once)
#!pip -q install gensim scikit-learn nltk matplotlib


# Imports
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_20newsgroups
from sklearn.decomposition import PCA

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from gensim.models import Word2Vec, FastText


# Download NLTK resources (run once per environment)
nltk.download("punkt")
nltk.download("stopwords")





# TODO: Load the dataset
# Use fetch_20newsgroups with subset="train" and remove=("headers", "footers", "quotes")
# Store the data in a variable called 'data' and documents in 'documents'

data = fetch_20newsgroups(subset="train", remove=("headers", "footers", "quotes"))  # YOUR CODE HERE
documents = data  # YOUR CODE HERE

print("Number of documents:", len(documents))
print("\nExample document snippet:\n")
print(documents[0][:600])





# Build stopword set once
stop_words = set(stopwords.words("english"))

def preprocess(text: str):
    """Convert raw text into a list of clean tokens.

    Steps:
    1) Lowercase
    2) Tokenize
    3) Keep alphabetic tokens only (drop numbers/punctuation)
    4) Remove stopwords

    Returns:
        List[str]: cleaned tokens
    """
    # TODO: Implement the preprocessing steps
    # 1. Lowercase and tokenize using word_tokenize
    # 2. Filter to keep only alphabetic tokens that are not stopwords
    
    tokens = word_tokenize(text.lower())  # YOUR CODE HERE
    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]  # YOUR CODE HERE (filter step)
    return tokens

# Tokenize the full corpus
corpus = [preprocess(doc) for doc in documents]

# Quick sanity check
print("Tokens in first document (first 50):")
print(corpus[0][:50])











# TODO: Train Word2Vec (Skip-gram)
# Use: sentences=corpus, vector_size=100, window=5, min_count=5, workers=4, sg=1

w2v = None  # YOUR CODE HERE

print("Vocabulary size:", len(w2v.wv))





# TODO: Inspect the first 10 values of the word vector for "computer"
# Use w2v.wv["computer"] to access the vector

# YOUR CODE HERE








# TODO: Find most similar words to "computer"
# Use w2v.wv.most_similar(target, topn=10)

target = "computer"
print(f"Most similar words to '{target}':")

# YOUR CODE HERE - iterate through the results and print each word and score








# TODO: Perform analogy using vector arithmetic
# Use w2v.wv.most_similar(positive=[...], negative=[...], topn=5)
# Try: king - man + woman = ?

# YOUR CODE HERE





# Check if "queen" is in the vocabulary
"queen" in w2v.wv


# TODO: Calculate the cosine similarity between the result vector and "queen"
# 1. Compute result_vec = w2v.wv["king"] - w2v.wv["man"] + w2v.wv["woman"]
# 2. Get queen_vec = w2v.wv["queen"]
# 3. Calculate cosine similarity using: np.dot(a, b) / (norm(a) * norm(b))

from numpy.linalg import norm

result_vec = None  # YOUR CODE HERE
queen_vec = None  # YOUR CODE HERE

similarity = None  # YOUR CODE HERE
print(f"Cosine similarity to 'queen': {similarity:.4f}")








# TODO: Visualize word embeddings using PCA
# 1. Define a list of words to visualize
# 2. Filter to keep only words in vocabulary
# 3. Get vectors for those words
# 4. Use PCA to reduce to 2D
# 5. Plot with labels

words = [
    "computer", "software", "hardware", "internet",
    "doctor", "nurse", "hospital", "medicine",
    "government", "policy", "law", "rights"
]

# Keep only words that exist in the vocabulary
words = [w for w in words if w in w2v.wv]

# TODO: Get vectors and apply PCA
vectors = None  # YOUR CODE HERE

pca = PCA(n_components=2)
coords = None  # YOUR CODE HERE

# Plot
plt.figure(figsize=(9, 6))
for i, w in enumerate(words):
    plt.scatter(coords[i, 0], coords[i, 1])
    plt.text(coords[i, 0] + 0.02, coords[i, 1] + 0.02, w)

plt.title("Word2Vec Embeddings Projected to 2D (PCA)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()








# TODO: Train FastText on the same corpus
# Use: sentences=corpus, vector_size=100, window=5, min_count=5, workers=4

ft = None  # YOUR CODE HERE

print("FastText vocabulary size:", len(ft.wv))





# TODO: Compare similarity results for "computers" between FastText and Word2Vec

query = "computers"

# FastText neighbors
print("FastText neighbors for:", query)
# YOUR CODE HERE

# Word2Vec neighbors (check if word exists first)
if query in w2v.wv:
    print("\nWord2Vec neighbors for:", query)
    # YOUR CODE HERE
else:
    print("\nWord2Vec does not contain the token 'computers' in its vocabulary.")








# TODO: Implement a semantic_neighbors function and test it with queries

def semantic_neighbors(model, word: str, topn: int = 10):
    """Return nearest neighbors for a word, with a friendly error message."""
    # YOUR CODE HERE
    # Check if word is in model.wv, if not return None
    # Otherwise return model.wv.most_similar(word, topn=topn)
    pass

queries = ["motel", "hotel", "space", "religion", "graphics"]

for q in queries:
    result = semantic_neighbors(w2v, q, topn=8)
    print("\nQuery:", q)
    if result is None:
        print("  (word not in vocabulary)")
    else:
        for w, s in result:
            print(f"  {w:15s} {s:.3f}")























# Imports
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_20newsgroups
from sklearn.decomposition import PCA

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from gensim.models import Word2Vec, FastText


# Download NLTK resources (run once per environment)
nltk.download("punkt")
nltk.download("stopwords")





# Load the dataset
# remove=... strips off headers/footers/quotes to reduce noise
data = fetch_20newsgroups(subset="train", remove=("headers", "footers", "quotes"))
documents = data.data

print("Number of documents:", len(documents))
print("\nExample document snippet:\n")
print(documents[0][:600])





# Build stopword set once
stop_words = set(stopwords.words("english"))

def preprocess(text: str):
    """Convert raw text into a list of clean tokens.

    Steps:
    1) Lowercase
    2) Tokenize
    3) Keep alphabetic tokens only (drop numbers/punctuation)
    4) Remove stopwords

    Returns:
        List[str]: cleaned tokens
    """
    tokens = word_tokenize(text.lower())
    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]  # isalpha() filters out non-alphabetic tokens
    return tokens

# Tokenize the full corpus
corpus = [preprocess(doc) for doc in documents]

# Quick sanity check
print("Tokens in first document (first 50):")
print(corpus[0][:50])











# Train Word2Vec (Skip-gram)
# workers uses multiple CPU cores to speed up training
w2v = Word2Vec(
    sentences=corpus,
    vector_size=100,   # embedding dimension
    window=5,          # context window
    min_count=5,       # ignore very rare words
    workers=4,
    sg=1               # 1 = Skip-gram, 0 = CBOW
)

print("Vocabulary size:", len(w2v.wv))





# Inspect the first few values of a word vector
w2v.wv["computer"][:10]








# Find most similar words to a target word
target = "computer"
print(f"Most similar words to '{target}':")
for word, score in w2v.wv.most_similar(target, topn=10): # w2v.wv.most_similar returns a list of (word, similarity score) tuples
    print(f"{word:15s}  {score:.3f}")








# Analogy / vector arithmetic
w2v.wv.most_similar(
    positive=["king", "woman"],
    negative=["man"],
    topn=5
)





"queen" in w2v.wv  # Returns True/False





# Check the actual similarity score for "queen"
import numpy as np
from numpy.linalg import norm

result_vec = w2v.wv["king"] - w2v.wv["man"] + w2v.wv["woman"]
queen_vec = w2v.wv["queen"]

similarity = np.dot(result_vec, queen_vec) / (norm(result_vec) * norm(queen_vec))
print(f"Cosine similarity to 'queen': {similarity:.4f}")








# Choose words to visualize (feel free to edit this list)
words = [
    "computer", "software", "hardware", "internet",
    "doctor", "nurse", "hospital", "medicine",
    "government", "policy", "law", "rights"
]

# Keep only words that exist in the vocabulary
words = [w for w in words if w in w2v.wv]

vectors = np.array([w2v.wv[w] for w in words])

# PCA to 2D
pca = PCA(n_components=2)
coords = pca.fit_transform(vectors)

# Plot
plt.figure(figsize=(9, 6))
for i, w in enumerate(words):
    plt.scatter(coords[i, 0], coords[i, 1])
    plt.text(coords[i, 0] + 0.02, coords[i, 1] + 0.02, w)

plt.title("Word2Vec Embeddings Projected to 2D (PCA)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()








# Train FastText on the same corpus
ft = FastText(
    sentences=corpus,
    vector_size=100,
    window=5,
    min_count=5,
    workers=4
)

print("FastText vocabulary size:", len(ft.wv))





# Compare similarity results for a variant form
query = "computers"

print("FastText neighbors for:", query)
for word, score in ft.wv.most_similar(query, topn=10):
    print(f"{word:15s}  {score:.3f}")

# Word2Vec may or may not have this token depending on min_count and preprocessing
if query in w2v.wv:
    print("\nWord2Vec neighbors for:", query)
    for word, score in w2v.wv.most_similar(query, topn=10):
        print(f"{word:15s}  {score:.3f}")
else:
    print("\nWord2Vec does not contain the token 'computers' in its vocabulary (likely filtered by min_count).")








def semantic_neighbors(model, word: str, topn: int = 10):
    """Return nearest neighbors for a word, with a friendly error message."""
    if word not in model.wv:
        return None
    return model.wv.most_similar(word, topn=topn)

queries = ["motel", "hotel", "space", "religion", "graphics"]

for q in queries:
    result = semantic_neighbors(w2v, q, topn=8)
    print("\nQuery:", q)
    if result is None:
        print("  (word not in vocabulary)")
    else:
        for w, s in result:
            print(f"  {w:15s} {s:.3f}")












